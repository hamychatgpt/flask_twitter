"""
Ø¢Ø²Ù…ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„Ú¯Ø± ØªÙˆÛŒÛŒØªØ± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² pytest
"""

import pytest
import os
import sys

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ù‡ sys.path Ø¨Ø±Ø§ÛŒ import Ú©Ø±Ø¯Ù† Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from twitter_analyzer.text_processing import (
    TextProcessor, 
    TextNormalizer, 
    TextTokenizer, 
    NoiseRemover,
    ContentFilter,
    BatchProcessor,
    get_stopwords
)

# Ù†Ù…ÙˆÙ†Ù‡ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
SAMPLE_TEXTS = [
    "Ø³Ù„Ø§Ù… Ø¯Ù†ÛŒØ§! Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ† ØªØ³Øª ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø³Øª. #ØªØ³Øª @Ù…Ø«Ø§Ù„ https://example.com",
    "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ù‡Ø´ØªÚ¯â€ŒÙ‡Ø§ÛŒ #Ø¨Ø±Ø±Ø³ÛŒ Ùˆ #Ø¢Ø²Ù…Ø§ÛŒØ´ Ø§Ø³Øª. Ù„Ø·ÙØ§ @Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ù…Ù†Ø´Ù† Ú©Ù†ÛŒØ¯ ðŸ˜Š",
    "RT @Ú©Ø§Ø±Ø¨Ø±: Ø§ÛŒÙ† ÛŒÚ© Ø±ÛŒØªÙˆÛŒÛŒØª Ø§Ø³Øª Ú©Ù‡ Ø´Ø§Ù…Ù„ Ù„ÛŒÙ†Ú© http://t.co/abc123 Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯",
    "Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ Ù…Ø§Ù†Ù†Ø¯ Ø§Ø­Ù…Ù‚ Ùˆ Ø¨ÛŒâ€ŒØ´Ø¹ÙˆØ±",
    "Ù„ÙˆØ±Ù… Ø§ÛŒÙ¾Ø³ÙˆÙ… Ù…ØªÙ† Ø³Ø§Ø®ØªÚ¯ÛŒ Ø¨Ø§ ØªÙˆÙ„ÛŒØ¯ Ø³Ø§Ø¯Ú¯ÛŒ Ù†Ø§Ù…ÙÙ‡ÙˆÙ… Ø§Ø² ØµÙ†Ø¹Øª Ú†Ø§Ù¾ Ùˆ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø·Ø±Ø§Ø­Ø§Ù† Ú¯Ø±Ø§ÙÛŒÚ© Ø§Ø³Øª. #Ù„ÙˆØ±Ù… #Ù…ØªÙ†"
]

@pytest.fixture
def text_processor():
    """ÙÛŒÚ©Ø³Ú†Ø± Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ TextProcessor"""
    config = {
        'language': 'fa',
        'normalizer': {
            'remove_diacritics': True,
            'affix_spacing': True,
            'punctuation_spacing': True
        },
        'noise_remover': {
            'preserve_emoji_length': False,
            'preserve_hashtag_content': True
        },
        'content_filter': {
            'censoring_char': '*',
            'min_inappropriate_threshold': 0.1
        }
    }
    
    return TextProcessor(config=config)

@pytest.fixture
def normalizer():
    """ÙÛŒÚ©Ø³Ú†Ø± Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ TextNormalizer"""
    return TextNormalizer()

@pytest.fixture
def tokenizer():
    """ÙÛŒÚ©Ø³Ú†Ø± Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ TextTokenizer"""
    return TextTokenizer(language='fa')

@pytest.fixture
def noise_remover():
    """ÙÛŒÚ©Ø³Ú†Ø± Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ NoiseRemover"""
    return NoiseRemover()

@pytest.fixture
def content_filter():
    """ÙÛŒÚ©Ø³Ú†Ø± Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ ContentFilter"""
    return ContentFilter()

@pytest.fixture
def batch_processor(text_processor):
    """ÙÛŒÚ©Ø³Ú†Ø± Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ BatchProcessor"""
    return BatchProcessor(text_processor=text_processor)

# Ø¢Ø²Ù…ÙˆÙ†â€ŒÙ‡Ø§ÛŒ TextNormalizer
def test_normalizer_initialization():
    """Ø¢Ø²Ù…ÙˆÙ† Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ TextNormalizer"""
    normalizer = TextNormalizer()
    assert normalizer is not None
    assert hasattr(normalizer, 'normalize')
    
def test_normalizer_normalize(normalizer):
    """Ø¢Ø²Ù…ÙˆÙ† Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
    for text in SAMPLE_TEXTS:
        normalized = normalizer.normalize(text)
        assert isinstance(normalized, str)
        assert len(normalized) > 0

# Ø¢Ø²Ù…ÙˆÙ†â€ŒÙ‡Ø§ÛŒ TextTokenizer
def test_tokenizer_initialization():
    """Ø¢Ø²Ù…ÙˆÙ† Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ TextTokenizer"""
    tokenizer = TextTokenizer(language='fa')
    assert tokenizer is not None
    assert tokenizer.language == 'fa'
    
def test_tokenizer_tokenize(tokenizer):
    """Ø¢Ø²Ù…ÙˆÙ† ØªÙˆÚ©Ù†ÛŒØ²Ù‡ Ú©Ø±Ø¯Ù† Ù…ØªÙ†"""
    for text in SAMPLE_TEXTS:
        tokens = tokenizer.tokenize(text)
        assert isinstance(tokens, list)
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø±Ø´ØªÙ‡ Ù‡Ø³ØªÙ†Ø¯
        assert all(isinstance(token, str) for token in tokens)
        
def test_tokenizer_sentences(tokenizer):
    """Ø¢Ø²Ù…ÙˆÙ† ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª"""
    for text in SAMPLE_TEXTS:
        sentences = tokenizer.tokenize_sentences(text)
        assert isinstance(sentences, list)
        assert all(isinstance(sentence, str) for sentence in sentences)

# Ø¢Ø²Ù…ÙˆÙ†â€ŒÙ‡Ø§ÛŒ NoiseRemover
def test_noise_remover_initialization():
    """Ø¢Ø²Ù…ÙˆÙ† Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ NoiseRemover"""
    noise_remover = NoiseRemover()
    assert noise_remover is not None
    assert hasattr(noise_remover, 'remove_all_noise')
    
def test_remove_urls(noise_remover):
    """Ø¢Ø²Ù…ÙˆÙ† Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§"""
    text_with_url = "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ ÛŒÚ© Ù„ÛŒÙ†Ú© https://example.com Ø§Ø³Øª"
    text_without_url = noise_remover.remove_urls(text_with_url)
    assert "https://example.com" not in text_without_url
    
def test_remove_mentions(noise_remover):
    """Ø¢Ø²Ù…ÙˆÙ† Ø­Ø°Ù Ù…Ù†Ø´Ù†â€ŒÙ‡Ø§"""
    text_with_mention = "Ù„Ø·ÙØ§ @Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ù…Ù†Ø´Ù† Ú©Ù†ÛŒØ¯"
    text_without_mention = noise_remover.remove_mentions(text_with_mention)
    assert "@Ú©Ø§Ø±Ø¨Ø±" not in text_without_mention
    
def test_remove_hashtags(noise_remover):
    """Ø¢Ø²Ù…ÙˆÙ† Ø­Ø°Ù Ù‡Ø´ØªÚ¯â€ŒÙ‡Ø§"""
    text_with_hashtag = "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ù‡Ø´ØªÚ¯ #ØªØ³Øª Ø§Ø³Øª"
    
    # Ø­Ø§Ù„Øª Ø­ÙØ¸ Ù…Ø­ØªÙˆØ§
    text_preserved = noise_remover.remove_hashtags(text_with_hashtag, preserve_content=True)
    assert "#ØªØ³Øª" not in text_preserved
    assert "ØªØ³Øª" in text_preserved
    
    # Ø­Ø§Ù„Øª Ø­Ø°Ù Ú©Ø§Ù…Ù„
    text_removed = noise_remover.remove_hashtags(text_with_hashtag, preserve_content=False)
    assert "#ØªØ³Øª" not in text_removed
    assert "ØªØ³Øª" not in text_removed
    
def test_remove_all_noise(noise_remover):
    """Ø¢Ø²Ù…ÙˆÙ† Ø­Ø°Ù ØªÙ…Ø§Ù… Ù†ÙˆÛŒØ²Ù‡Ø§"""
    text_with_noise = "RT @Ú©Ø§Ø±Ø¨Ø±: Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ† Ù†ÙˆÛŒØ²Ø¯Ø§Ø± #Ù‡Ø´ØªÚ¯ Ø¨Ø§ Ù„ÛŒÙ†Ú© https://example.com Ø§Ø³Øª ðŸ˜Š"
    text_without_noise = noise_remover.remove_all_noise(text_with_noise)
    
    assert "RT" not in text_without_noise
    assert "@Ú©Ø§Ø±Ø¨Ø±" not in text_without_noise
    assert "#Ù‡Ø´ØªÚ¯" not in text_without_noise
    assert "https://example.com" not in text_without_noise
    assert "ðŸ˜Š" not in text_without_noise

# Ø¢Ø²Ù…ÙˆÙ†â€ŒÙ‡Ø§ÛŒ ContentFilter
def test_content_filter_initialization():
    """Ø¢Ø²Ù…ÙˆÙ† Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ContentFilter"""
    content_filter = ContentFilter()
    assert content_filter is not None
    assert hasattr(content_filter, 'is_appropriate')
    
def test_inappropriate_words(content_filter):
    """Ø¢Ø²Ù…ÙˆÙ† ØªØ´Ø®ÛŒØµ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ù…Ù†Ø§Ø³Ø¨"""
    clean_text = "Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ† Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª"
    inappropriate_text = "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú©Ù„Ù…Ù‡ Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ Ø§Ø­Ù…Ù‚ Ø§Ø³Øª"
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨ÙˆØ¯Ù†
    assert content_filter.is_appropriate(clean_text)
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ Ø¨ÙˆØ¯Ù†
    inappropriate_words = content_filter.get_inappropriate_words(inappropriate_text)
    assert len(inappropriate_words) > 0
    assert "Ø§Ø­Ù…Ù‚" in inappropriate_words
    
def test_filter_text(content_filter):
    """Ø¢Ø²Ù…ÙˆÙ† ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ù†Ø§Ù…Ù†Ø§Ø³Ø¨"""
    inappropriate_text = "Ø§ÛŒÙ† Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ú©Ù„Ù…Ù‡ Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ Ø§Ø­Ù…Ù‚ Ø§Ø³Øª"
    
    # Ø³Ø§Ù†Ø³ÙˆØ±
    censored_text = content_filter.filter_text(inappropriate_text, replacement_method='censor')
    assert "Ø§Ø­Ù…Ù‚" not in censored_text
    assert "****" in censored_text
    
    # Ø­Ø°Ù
    removed_text = content_filter.filter_text(inappropriate_text, replacement_method='remove')
    assert "Ø§Ø­Ù…Ù‚" not in removed_text
    assert "****" not in removed_text

# Ø¢Ø²Ù…ÙˆÙ†â€ŒÙ‡Ø§ÛŒ TextProcessor
def test_text_processor_initialization():
    """Ø¢Ø²Ù…ÙˆÙ† Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ TextProcessor"""
    processor = TextProcessor()
    assert processor is not None
    assert hasattr(processor, 'preprocess')
    
def test_preprocess(text_processor):
    """Ø¢Ø²Ù…ÙˆÙ† Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†"""
    for text in SAMPLE_TEXTS:
        preprocessed = text_processor.preprocess(text)
        assert isinstance(preprocessed, str)
        
def test_extract_features(text_processor):
    """Ø¢Ø²Ù…ÙˆÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§"""
    text = "Ø³Ù„Ø§Ù… Ø¯Ù†ÛŒØ§! Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ† ØªØ³Øª #Ø¢Ø²Ù…Ø§ÛŒØ´ Ø¨Ø±Ø§ÛŒ @Ú©Ø§Ø±Ø¨Ø± Ø§Ø³Øª https://example.com ðŸ˜Š"
    features = text_processor.extract_features(text)
    
    assert isinstance(features, dict)
    assert 'length' in features
    assert 'word_count' in features
    assert 'mentions' in features
    assert 'hashtags' in features
    assert 'urls' in features
    assert 'emojis' in features
    
    assert features['length'] > 0
    assert features['word_count'] > 0
    assert '@Ú©Ø§Ø±Ø¨Ø±' in ' '.join(features['mentions']) or 'Ú©Ø§Ø±Ø¨Ø±' in ' '.join(features['mentions'])
    assert '#Ø¢Ø²Ù…Ø§ÛŒØ´' in ' '.join(features['hashtags']) or 'Ø¢Ø²Ù…Ø§ÛŒØ´' in ' '.join(features['hashtags'])
    assert 'https://example.com' in ' '.join(features['urls'])
    
def test_get_keywords(text_processor):
    """Ø¢Ø²Ù…ÙˆÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ"""
    text = "Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ† Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø³Øª. Ù…ØªÙ† Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø´Ø§Ù…Ù„ Ú©Ù„Ù…Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø§Ø³Øª."
    keywords = text_processor.get_keywords(text, top_n=3)
    
    assert isinstance(keywords, list)
    assert len(keywords) <= 3  # Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©Ù…ØªØ± Ø§Ø² 3 Ø¨Ø§Ø´Ø¯ Ø§Ú¯Ø± Ú©Ù„Ù…Ø§Øª Ú©Ø§ÙÛŒ Ù†Ø¨Ø§Ø´Ù†Ø¯
    assert all(isinstance(item, tuple) and len(item) == 2 for item in keywords)

# Ø¢Ø²Ù…ÙˆÙ†â€ŒÙ‡Ø§ÛŒ BatchProcessor
def test_batch_processor_initialization(text_processor):
    """Ø¢Ø²Ù…ÙˆÙ† Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ BatchProcessor"""
    batch_processor = BatchProcessor(text_processor=text_processor)
    assert batch_processor is not None
    assert batch_processor.text_processor == text_processor
    
def test_process_texts(batch_processor):
    """Ø¢Ø²Ù…ÙˆÙ† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ù…ØªÙˆÙ†"""
    results = batch_processor.process_texts(
        SAMPLE_TEXTS[:2],  # ÙÙ‚Ø· Ø¯Ùˆ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±
        preprocess=True,
        extract_features=True,
        filter_inappropriate=True
    )
    
    assert isinstance(results, list)
    assert len(results) == 2
    assert all(isinstance(result, dict) for result in results)
    assert all('original' in result for result in results)
    
def test_get_stats(batch_processor):
    """Ø¢Ø²Ù…ÙˆÙ† Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´"""
    # Ø§Ø¨ØªØ¯Ø§ ÛŒÚ© Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
    batch_processor.process_texts(SAMPLE_TEXTS[:2])
    
    # Ø³Ù¾Ø³ Ø¢Ù…Ø§Ø± Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    stats = batch_processor.get_stats()
    
    assert isinstance(stats, dict)
    assert 'total_items' in stats
    assert 'processed_items' in stats
    assert 'processing_time' in stats
    assert stats['total_items'] == 2  # ØªØ¹Ø¯Ø§Ø¯ Ù…ØªÙˆÙ† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
    assert stats['processed_items'] == 2  # ØªØ¹Ø¯Ø§Ø¯ Ù…ØªÙˆÙ† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡